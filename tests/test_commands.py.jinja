"""Tests for command modules."""

import json
from pathlib import Path
from typer.testing import CliRunner

from {{ package_name }}.cli import app


class TestDataCommands:
    """Tests for data processing commands."""

    def test_data_list_current_directory(self):
        """Test listing files in current directory."""
        runner = CliRunner()
        result = runner.invoke(app, ["data", "list", "."])

        assert result.exit_code == 0
        # Should show some output about files
        assert len(result.stdout.strip()) > 0

    def test_data_list_with_hidden_files(self, sample_directory_structure):
        """Test listing files including hidden ones."""
        runner = CliRunner()
        result = runner.invoke(app, ["data", "list", str(sample_directory_structure), "--hidden"])

        assert result.exit_code == 0
        # Should include hidden files in output
        assert ".hidden" in result.stdout or "hidden" in result.stdout

    def test_data_list_without_hidden_files(self, sample_directory_structure):
        """Test listing files excluding hidden ones."""
        runner = CliRunner()
        result = runner.invoke(app, ["data", "list", str(sample_directory_structure)])

        assert result.exit_code == 0

    def test_data_process_json_file(self, sample_json_file):
        """Test processing a JSON file."""
        runner = CliRunner()
        result = runner.invoke(app, ["data", "process", str(sample_json_file)])

        assert result.exit_code == 0
        # Output should be valid JSON
        try:
            output_data = json.loads(result.stdout)
            assert "data" in output_data
            assert "analysis" in output_data
        except json.JSONDecodeError:
            # If not JSON output, check for success indicators
            assert "Error" not in result.stdout

    def test_data_process_csv_file(self, sample_csv_file):
        """Test processing a CSV file."""
        runner = CliRunner()
        result = runner.invoke(app, ["data", "process", str(sample_csv_file)])

        assert result.exit_code == 0
        assert "Error" not in result.stdout

    def test_data_process_text_file(self, sample_text_file):
        """Test processing a text file."""
        runner = CliRunner()
        result = runner.invoke(app, ["data", "process", str(sample_text_file)])

        assert result.exit_code == 0
        assert "Error" not in result.stdout

    def test_data_process_with_output_file(self, sample_json_file, temp_dir):
        """Test processing with output file."""
        output_file = temp_dir / "output.json"
        runner = CliRunner()
        result = runner.invoke(app, [
            "data", "process", str(sample_json_file),
            "--output", str(output_file)
        ])

        assert result.exit_code == 0
        assert output_file.exists()
        # Should contain valid JSON
        output_data = json.loads(output_file.read_text())
        assert "data" in output_data

    def test_data_process_csv_format(self, sample_json_file):
        """Test processing with CSV output format."""
        runner = CliRunner()
        result = runner.invoke(app, [
            "data", "process", str(sample_json_file),
            "--format", "csv"
        ])

        assert result.exit_code == 0
        # Should contain CSV-like output
        assert "," in result.stdout or "key,value" in result.stdout

    def test_data_process_yaml_format(self, sample_json_file):
        """Test processing with YAML output format."""
        runner = CliRunner()
        result = runner.invoke(app, [
            "data", "process", str(sample_json_file),
            "--format", "yaml"
        ])

        assert result.exit_code == 0
        # Should contain YAML-like output
        assert ":" in result.stdout

    def test_data_process_verbose(self, sample_json_file):
        """Test processing with verbose output."""
        runner = CliRunner()
        result = runner.invoke(app, [
            "data", "process", str(sample_json_file),
            "--verbose"
        ])

        assert result.exit_code == 0

    def test_data_process_invalid_file(self):
        """Test processing non-existent file."""
        runner = CliRunner()
        result = runner.invoke(app, ["data", "process", "/nonexistent/file.json"])

        assert result.exit_code != 0
        assert "does not exist" in result.stdout


{%- if include_config %}
class TestConfigCommands:
    """Tests for configuration commands."""

    def test_config_init(self, temp_dir):
        """Test configuration initialization."""
        config_path = temp_dir / "config.toml"
        runner = CliRunner()

        # Mock the config path by setting environment
        import os
        original_home = os.environ.get("HOME")
        os.environ["HOME"] = str(temp_dir)

        try:
            result = runner.invoke(app, ["config", "init"])
            # Note: This might fail in real execution due to path issues
            # but should at least not crash
            assert result.exit_code in (0, 1)  # Allow both success and expected failure
        finally:
            if original_home:
                os.environ["HOME"] = original_home

    def test_config_show_nonexistent(self):
        """Test showing non-existent configuration."""
        runner = CliRunner()
        result = runner.invoke(app, ["config", "show", "--path", "/nonexistent/config.toml"])

        assert result.exit_code != 0
        assert "not found" in result.stdout

    def test_config_show_existing(self, config_file):
        """Test showing existing configuration."""
        runner = CliRunner()
        result = runner.invoke(app, ["config", "show", "--path", str(config_file)])

        assert result.exit_code == 0
        assert "general" in result.stdout or "config" in result.stdout.lower()

    def test_config_validate_valid(self, config_file):
        """Test validating valid configuration."""
        runner = CliRunner()
        result = runner.invoke(app, ["config", "validate", "--path", str(config_file)])

        assert result.exit_code == 0
        assert "valid" in result.stdout.lower()

    def test_config_validate_invalid(self, temp_dir):
        """Test validating invalid configuration."""
        invalid_config = temp_dir / "invalid.toml"
        invalid_config.write_text("""
[general]
output_format = "invalid_format"
verbose = "not_a_boolean"
""")

        runner = CliRunner()
        result = runner.invoke(app, ["config", "validate", "--path", str(invalid_config)])

        assert result.exit_code != 0
        assert "validation failed" in result.stdout.lower() or "error" in result.stdout.lower()

    def test_config_init_force_overwrite(self, temp_dir):
        """Test configuration init with force flag."""
        config_path = temp_dir / "config.toml"
        config_path.write_text("[existing]\ndata = true")

        runner = CliRunner()

        # Mock the config path
        import os
        original_home = os.environ.get("HOME")
        os.environ["HOME"] = str(temp_dir)

        try:
            result = runner.invoke(app, ["config", "init", "--force"])
            # Allow both success and expected failure due to path handling
            assert result.exit_code in (0, 1)
        finally:
            if original_home:
                os.environ["HOME"] = original_home
{%- endif %}


class TestCommandIntegration:
    """Integration tests across commands."""

    def test_end_to_end_workflow(self, sample_directory_structure, temp_dir):
        """Test a complete workflow using multiple commands."""
        runner = CliRunner()

        # 1. List the directory
        result1 = runner.invoke(app, ["data", "list", str(sample_directory_structure)])
        assert result1.exit_code == 0

        # 2. Process a file if it exists
        json_file = sample_directory_structure / "subdir1" / "nested.json"
        if json_file.exists():
            result2 = runner.invoke(app, ["data", "process", str(json_file)])
            assert result2.exit_code == 0

    def test_error_handling_chain(self):
        """Test error handling across different commands."""
        runner = CliRunner()

        # Test various error conditions
        error_tests = [
            (["data", "list", "/nonexistent"], "does not exist"),
            (["data", "process", "/nonexistent.json"], "does not exist"),
            (["data", "process", "dummy.txt", "--format", "invalid"], "Unsupported format"),
        ]

        for command, expected_error in error_tests:
            result = runner.invoke(app, command)
            assert result.exit_code != 0
            assert expected_error.lower() in result.stdout.lower()

    def test_help_consistency(self):
        """Test that help messages are consistent across commands."""
        runner = CliRunner()

        help_commands = [
            ["--help"],
            ["data", "--help"],
            ["data", "list", "--help"],
            ["data", "process", "--help"],
        ]

{%- if include_config %}
        help_commands.extend([
            ["config", "--help"],
            ["config", "init", "--help"],
            ["config", "show", "--help"],
            ["config", "validate", "--help"],
        ])
{%- endif %}

        for command in help_commands:
            result = runner.invoke(app, command)
            assert result.exit_code == 0
            assert "Usage:" in result.stdout or "help" in result.stdout.lower()